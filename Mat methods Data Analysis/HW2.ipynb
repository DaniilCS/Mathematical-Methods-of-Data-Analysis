{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "hw2 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IvoZ9L35hVi"
      },
      "source": [
        "# HSE 2021: Mathematical Methods for Data Analysis\n",
        "\n",
        "## Homework 2\n",
        "\n",
        "### Attention!\n",
        "* For tasks where <ins>text answer</ins> is required **Russian language** is **allowed**.\n",
        "* If a task asks you to describe something (make coclusions) then **text answer** is **mandatory** and **is** part of the task\n",
        "* We **only** accept **ipynb** notebooks. If you use Google Colab then you'll have to download the notebook before passing the homework\n",
        "* **Do not** use python loops instead of NumPy vector operations over NumPy vectors - it significantly decreases performance (see why https://blog.paperspace.com/numpy-optimization-vectorization-and-broadcasting/), will be punished with -0.25 for **every** task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-26T16:48:20.566549Z",
          "start_time": "2020-09-26T16:48:19.893995Z"
        },
        "id": "gbV84WHl5hVm"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set(style=\"darkgrid\")"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aiRrjDM5hVn"
      },
      "source": [
        "### Data\n",
        "\n",
        "For this homework we use Boston Dataset from sklearn (based on UCI ML housing dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajO2EBoD5hVo"
      },
      "source": [
        "data = load_boston() # load dataset\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "columns = data.feature_names"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-re-xLb5hVo"
      },
      "source": [
        "## Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQh9Fku05hVp"
      },
      "source": [
        "#### 1. [0.5 points] \n",
        "Create Pandas DataFrame and split the data into train and test sets with ratio 80:20 with random_state=0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "NbVm_DCe5hVp",
        "outputId": "19bb36ad-06ad-4108-d9f8-7f018939ab74"
      },
      "source": [
        "# your code here \n",
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train = pd.DataFrame(data = X_train, columns= columns)\n",
        "X_train.head()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.35809</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.507</td>\n",
              "      <td>6.951</td>\n",
              "      <td>88.5</td>\n",
              "      <td>2.8617</td>\n",
              "      <td>8.0</td>\n",
              "      <td>307.0</td>\n",
              "      <td>17.4</td>\n",
              "      <td>391.70</td>\n",
              "      <td>9.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.15876</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.413</td>\n",
              "      <td>5.961</td>\n",
              "      <td>17.5</td>\n",
              "      <td>5.2873</td>\n",
              "      <td>4.0</td>\n",
              "      <td>305.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>376.94</td>\n",
              "      <td>9.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.11329</td>\n",
              "      <td>30.0</td>\n",
              "      <td>4.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.428</td>\n",
              "      <td>6.897</td>\n",
              "      <td>54.3</td>\n",
              "      <td>6.3361</td>\n",
              "      <td>6.0</td>\n",
              "      <td>300.0</td>\n",
              "      <td>16.6</td>\n",
              "      <td>391.25</td>\n",
              "      <td>11.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.08829</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.012</td>\n",
              "      <td>66.6</td>\n",
              "      <td>5.5605</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>395.60</td>\n",
              "      <td>12.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25.94060</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.679</td>\n",
              "      <td>5.304</td>\n",
              "      <td>89.1</td>\n",
              "      <td>1.6475</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>127.36</td>\n",
              "      <td>26.64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       CRIM    ZN  INDUS  CHAS    NOX  ...   RAD    TAX  PTRATIO       B  LSTAT\n",
              "0   0.35809   0.0   6.20   1.0  0.507  ...   8.0  307.0     17.4  391.70   9.71\n",
              "1   0.15876   0.0  10.81   0.0  0.413  ...   4.0  305.0     19.2  376.94   9.88\n",
              "2   0.11329  30.0   4.93   0.0  0.428  ...   6.0  300.0     16.6  391.25  11.38\n",
              "3   0.08829  12.5   7.87   0.0  0.524  ...   5.0  311.0     15.2  395.60  12.43\n",
              "4  25.94060   0.0  18.10   0.0  0.679  ...  24.0  666.0     20.2  127.36  26.64\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDssBzvp5hVp"
      },
      "source": [
        "---\n",
        "#### 2. [1 point] \n",
        "Train models on train data using StatsModels( or sckit-learn) library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
        "\n",
        "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
        "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
        "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
        "\n",
        "Don't forget to scale the data before training the models with StandardScaler!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "291M4TH75hVq",
        "outputId": "125df9f9-af29-439e-8434-12554ef330eb"
      },
      "source": [
        "# your code here \n",
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaller = StandardScaler()\n",
        "X_train = scaller.fit_transform(X_train)\n",
        "X_test = scaller.transform(X_test)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_rg = LinearRegression()\n",
        "lin_rg.fit(X_train, y_train)\n",
        "pred = lin_rg.predict(X_test)\n",
        "print(r2_score(pred, y_test))\n",
        "print(mean_squared_error(pred, y_test) ** 0.5, '\\n')\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge = Ridge(alpha = 0.5)\n",
        "ridge.fit(X_train, y_train)\n",
        "pred = ridge.predict(X_test)\n",
        "print(r2_score(pred, y_test))\n",
        "print(mean_squared_error(pred, y_test) ** 0.5, '\\n')\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha = 0.5)\n",
        "lasso.fit(X_train, y_train)\n",
        "pred = lasso.predict(X_test)\n",
        "print(r2_score(pred, y_test))\n",
        "print(mean_squared_error(pred, y_test) ** 0.5, '\\n')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3523653008788691\n",
            "5.783509315085134 \n",
            "\n",
            "0.3511027436346723\n",
            "5.78733207930106 \n",
            "\n",
            "0.14400798004973614\n",
            "6.304209093307778 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO7KWs2V5hVr"
      },
      "source": [
        "---\n",
        "#### 3. [1 point] \n",
        "Explore the values of the parameters of the resulting models and compare the number of zero weights in them. \n",
        "\n",
        "Comment on the significance of the coefficients, overal model significance and other related factors from the results table. \n",
        "\n",
        "`Hint` Use StatModels to obtain significance of the coefficients. They ca be found on the `summary` of the fitted linear model. \n",
        "It might be tricky to obtain `summary` for the regularized model. Please, read the documentation of the StatModels library to figure out how to do that, e.g.   [OLSResults](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLSResults.html#statsmodels.regression.linear_model.OLSResults) class might be useful here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "w49HA9wF5hVr",
        "outputId": "650fba17-f9ca-47d3-e012-15c57d3b54ff"
      },
      "source": [
        "# your code here \n",
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "from statsmodels.regression.linear_model import OLSResults\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.regression.linear_model import OLSResults\n",
        "model = sm.OLS(y_train, X_train)\n",
        "results = model.fit()\n",
        "results.summary2()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "          <td>Model:</td>                 <td>OLS</td>       <td>Adj. R-squared (uncentered):</td>   <td>0.081</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "    <td>Dependent Variable:</td>           <td>y</td>                    <td>AIC:</td>             <td>3707.2197</td>\n",
              "</tr>\n",
              "<tr>\n",
              "           <td>Date:</td>          <td>2021-10-11 18:36</td>             <td>BIC:</td>             <td>3759.2381</td>\n",
              "</tr>\n",
              "<tr>\n",
              "     <td>No. Observations:</td>           <td>404</td>              <td>Log-Likelihood:</td>        <td>-1840.6</td> \n",
              "</tr>\n",
              "<tr>\n",
              "         <td>Df Model:</td>               <td>13</td>                <td>F-statistic:</td>           <td>3.731</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "       <td>Df Residuals:</td>             <td>391</td>            <td>Prob (F-statistic):</td>     <td>1.24e-05</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <td>R-squared (uncentered):</td>       <td>0.110</td>                 <td>Scale:</td>             <td>548.27</td>  \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "   <td></td>    <th>Coef.</th>  <th>Std.Err.</th>    <th>t</th>     <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x1</th>  <td>-0.9708</td>  <td>1.5596</td>  <td>-0.6225</td> <td>0.5340</td> <td>-4.0371</td> <td>2.0955</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x2</th>  <td>1.0571</td>   <td>1.7833</td>  <td>0.5928</td>  <td>0.5537</td> <td>-2.4489</td> <td>4.5632</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x3</th>  <td>0.0383</td>   <td>2.3175</td>  <td>0.0165</td>  <td>0.9868</td> <td>-4.5180</td> <td>4.5946</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x4</th>  <td>0.5945</td>   <td>1.1991</td>  <td>0.4958</td>  <td>0.6203</td> <td>-1.7629</td> <td>2.9519</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x5</th>  <td>-1.8551</td>  <td>2.5360</td>  <td>-0.7315</td> <td>0.4649</td> <td>-6.8410</td> <td>3.1307</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x6</th>  <td>2.5732</td>   <td>1.6613</td>  <td>1.5489</td>  <td>0.1222</td> <td>-0.6929</td> <td>5.8394</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x7</th>  <td>-0.0876</td>  <td>2.1050</td>  <td>-0.0416</td> <td>0.9668</td> <td>-4.2261</td> <td>4.0509</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x8</th>  <td>-2.8809</td>  <td>2.3266</td>  <td>-1.2383</td> <td>0.2164</td> <td>-7.4552</td> <td>1.6933</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x9</th>  <td>2.1122</td>   <td>3.1759</td>  <td>0.6651</td>  <td>0.5064</td> <td>-4.1317</td> <td>8.3562</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x10</th> <td>-1.8753</td>  <td>3.4812</td>  <td>-0.5387</td> <td>0.5904</td> <td>-8.7196</td> <td>4.9689</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x11</th> <td>-2.2928</td>  <td>1.5713</td>  <td>-1.4591</td> <td>0.1453</td> <td>-5.3820</td> <td>0.7965</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x12</th> <td>0.7182</td>   <td>1.3674</td>  <td>0.5252</td>  <td>0.5997</td> <td>-1.9701</td> <td>3.4065</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x13</th> <td>-3.5925</td>  <td>2.0692</td>  <td>-1.7362</td> <td>0.0833</td> <td>-7.6606</td> <td>0.4757</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "     <td>Omnibus:</td>    <td>141.494</td>  <td>Durbin-Watson:</td>    <td>0.073</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <td>Prob(Omnibus):</td>  <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>629.882</td>\n",
              "</tr>\n",
              "<tr>\n",
              "       <td>Skew:</td>      <td>1.470</td>      <td>Prob(JB):</td>      <td>0.000</td> \n",
              "</tr>\n",
              "<tr>\n",
              "     <td>Kurtosis:</td>    <td>8.365</td>   <td>Condition No.:</td>     <td>10</td>   \n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary2.Summary'>\n",
              "\"\"\"\n",
              "                        Results: Ordinary least squares\n",
              "===============================================================================\n",
              "Model:                  OLS              Adj. R-squared (uncentered): 0.081    \n",
              "Dependent Variable:     y                AIC:                         3707.2197\n",
              "Date:                   2021-10-11 18:36 BIC:                         3759.2381\n",
              "No. Observations:       404              Log-Likelihood:              -1840.6  \n",
              "Df Model:               13               F-statistic:                 3.731    \n",
              "Df Residuals:           391              Prob (F-statistic):          1.24e-05 \n",
              "R-squared (uncentered): 0.110            Scale:                       548.27   \n",
              "------------------------------------------------------------------------------------\n",
              "              Coef.       Std.Err.         t         P>|t|        [0.025      0.975]\n",
              "------------------------------------------------------------------------------------\n",
              "x1           -0.9708        1.5596      -0.6225      0.5340      -4.0371      2.0955\n",
              "x2            1.0571        1.7833       0.5928      0.5537      -2.4489      4.5632\n",
              "x3            0.0383        2.3175       0.0165      0.9868      -4.5180      4.5946\n",
              "x4            0.5945        1.1991       0.4958      0.6203      -1.7629      2.9519\n",
              "x5           -1.8551        2.5360      -0.7315      0.4649      -6.8410      3.1307\n",
              "x6            2.5732        1.6613       1.5489      0.1222      -0.6929      5.8394\n",
              "x7           -0.0876        2.1050      -0.0416      0.9668      -4.2261      4.0509\n",
              "x8           -2.8809        2.3266      -1.2383      0.2164      -7.4552      1.6933\n",
              "x9            2.1122        3.1759       0.6651      0.5064      -4.1317      8.3562\n",
              "x10          -1.8753        3.4812      -0.5387      0.5904      -8.7196      4.9689\n",
              "x11          -2.2928        1.5713      -1.4591      0.1453      -5.3820      0.7965\n",
              "x12           0.7182        1.3674       0.5252      0.5997      -1.9701      3.4065\n",
              "x13          -3.5925        2.0692      -1.7362      0.0833      -7.6606      0.4757\n",
              "-------------------------------------------------------------------------------\n",
              "Omnibus:                  141.494           Durbin-Watson:              0.073  \n",
              "Prob(Omnibus):            0.000             Jarque-Bera (JB):           629.882\n",
              "Skew:                     1.470             Prob(JB):                   0.000  \n",
              "Kurtosis:                 8.365             Condition No.:              10     \n",
              "===============================================================================\n",
              "\n",
              "\"\"\""
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "GpiefIMArIWU",
        "outputId": "1c64e383-d01e-433f-cbd2-0f33c0fb6caa"
      },
      "source": [
        "final = sm.regression.linear_model.OLSResults(model, \n",
        "                                              ridge.coef_, \n",
        "                                              model.normalized_cov_params)\n",
        "final.summary()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.110</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.081</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   3.731</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Mon, 11 Oct 2021</td> <th>  Prob (F-statistic):</th>          <td>1.24e-05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>18:36:47</td>     <th>  Log-Likelihood:    </th>          <td> -1840.6</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>   404</td>      <th>  AIC:               </th>          <td>   3707.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>   391</td>      <th>  BIC:               </th>          <td>   3759.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>              <td> </td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x1</th>  <td>   -0.9665</td> <td>    1.560</td> <td>   -0.620</td> <td> 0.536</td> <td>   -4.033</td> <td>    2.100</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x2</th>  <td>    1.0489</td> <td>    1.783</td> <td>    0.588</td> <td> 0.557</td> <td>   -2.457</td> <td>    4.555</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x3</th>  <td>    0.0247</td> <td>    2.317</td> <td>    0.011</td> <td> 0.991</td> <td>   -4.532</td> <td>    4.581</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x4</th>  <td>    0.5967</td> <td>    1.199</td> <td>    0.498</td> <td> 0.619</td> <td>   -1.761</td> <td>    2.954</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x5</th>  <td>   -1.8375</td> <td>    2.536</td> <td>   -0.725</td> <td> 0.469</td> <td>   -6.823</td> <td>    3.148</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x6</th>  <td>    2.5786</td> <td>    1.661</td> <td>    1.552</td> <td> 0.121</td> <td>   -0.688</td> <td>    5.845</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x7</th>  <td>   -0.0915</td> <td>    2.105</td> <td>   -0.043</td> <td> 0.965</td> <td>   -4.230</td> <td>    4.047</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x8</th>  <td>   -2.8645</td> <td>    2.327</td> <td>   -1.231</td> <td> 0.219</td> <td>   -7.439</td> <td>    1.710</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x9</th>  <td>    2.0736</td> <td>    3.176</td> <td>    0.653</td> <td> 0.514</td> <td>   -4.170</td> <td>    8.317</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x10</th> <td>   -1.8400</td> <td>    3.481</td> <td>   -0.529</td> <td> 0.597</td> <td>   -8.684</td> <td>    5.004</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x11</th> <td>   -2.2879</td> <td>    1.571</td> <td>   -1.456</td> <td> 0.146</td> <td>   -5.377</td> <td>    0.801</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x12</th> <td>    0.7182</td> <td>    1.367</td> <td>    0.525</td> <td> 0.600</td> <td>   -1.970</td> <td>    3.407</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x13</th> <td>   -3.5842</td> <td>    2.069</td> <td>   -1.732</td> <td> 0.084</td> <td>   -7.652</td> <td>    0.484</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>142.324</td> <th>  Durbin-Watson:     </th> <td>   0.073</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 638.431</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td> 1.476</td>  <th>  Prob(JB):          </th> <td>2.33e-139</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td> 8.404</td>  <th>  Cond. No.          </th> <td>    9.81</td> \n",
              "</tr>\n",
              "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                                 OLS Regression Results                                \n",
              "=======================================================================================\n",
              "Dep. Variable:                      y   R-squared (uncentered):                   0.110\n",
              "Model:                            OLS   Adj. R-squared (uncentered):              0.081\n",
              "Method:                 Least Squares   F-statistic:                              3.731\n",
              "Date:                Mon, 11 Oct 2021   Prob (F-statistic):                    1.24e-05\n",
              "Time:                        18:36:47   Log-Likelihood:                         -1840.6\n",
              "No. Observations:                 404   AIC:                                      3707.\n",
              "Df Residuals:                     391   BIC:                                      3759.\n",
              "Df Model:                          13                                                  \n",
              "Covariance Type:            nonrobust                                                  \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "x1            -0.9665      1.560     -0.620      0.536      -4.033       2.100\n",
              "x2             1.0489      1.783      0.588      0.557      -2.457       4.555\n",
              "x3             0.0247      2.317      0.011      0.991      -4.532       4.581\n",
              "x4             0.5967      1.199      0.498      0.619      -1.761       2.954\n",
              "x5            -1.8375      2.536     -0.725      0.469      -6.823       3.148\n",
              "x6             2.5786      1.661      1.552      0.121      -0.688       5.845\n",
              "x7            -0.0915      2.105     -0.043      0.965      -4.230       4.047\n",
              "x8            -2.8645      2.327     -1.231      0.219      -7.439       1.710\n",
              "x9             2.0736      3.176      0.653      0.514      -4.170       8.317\n",
              "x10           -1.8400      3.481     -0.529      0.597      -8.684       5.004\n",
              "x11           -2.2879      1.571     -1.456      0.146      -5.377       0.801\n",
              "x12            0.7182      1.367      0.525      0.600      -1.970       3.407\n",
              "x13           -3.5842      2.069     -1.732      0.084      -7.652       0.484\n",
              "==============================================================================\n",
              "Omnibus:                      142.324   Durbin-Watson:                   0.073\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              638.431\n",
              "Skew:                           1.476   Prob(JB):                    2.33e-139\n",
              "Kurtosis:                       8.404   Cond. No.                         9.81\n",
              "==============================================================================\n",
              "\n",
              "Warnings:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "DhiYSClfuCKA",
        "outputId": "4f17c18f-2c5d-4654-c54e-78a61499749b"
      },
      "source": [
        "final = sm.regression.linear_model.OLSResults(model, \n",
        "                                              lasso.coef_, \n",
        "                                              model.normalized_cov_params)\n",
        "final.summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.104</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.075</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   3.507</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Mon, 11 Oct 2021</td> <th>  Prob (F-statistic):</th>          <td>3.41e-05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>18:36:47</td>     <th>  Log-Likelihood:    </th>          <td> -1842.0</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>   404</td>      <th>  AIC:               </th>          <td>   3710.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>   391</td>      <th>  BIC:               </th>          <td>   3762.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>              <td> </td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x1</th>  <td>   -0.2469</td> <td>    1.565</td> <td>   -0.158</td> <td> 0.875</td> <td>   -3.323</td> <td>    2.830</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x2</th>  <td>         0</td> <td>    1.789</td> <td>        0</td> <td> 1.000</td> <td>   -3.518</td> <td>    3.518</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x3</th>  <td>        -0</td> <td>    2.325</td> <td>       -0</td> <td> 1.000</td> <td>   -4.571</td> <td>    4.571</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x4</th>  <td>    0.3420</td> <td>    1.203</td> <td>    0.284</td> <td> 0.776</td> <td>   -2.023</td> <td>    2.707</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x5</th>  <td>        -0</td> <td>    2.544</td> <td>       -0</td> <td> 1.000</td> <td>   -5.002</td> <td>    5.002</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x6</th>  <td>    2.8508</td> <td>    1.667</td> <td>    1.710</td> <td> 0.088</td> <td>   -0.426</td> <td>    6.128</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x7</th>  <td>        -0</td> <td>    2.112</td> <td>       -0</td> <td> 1.000</td> <td>   -4.152</td> <td>    4.152</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x8</th>  <td>        -0</td> <td>    2.334</td> <td>       -0</td> <td> 1.000</td> <td>   -4.589</td> <td>    4.589</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x9</th>  <td>        -0</td> <td>    3.186</td> <td>       -0</td> <td> 1.000</td> <td>   -6.265</td> <td>    6.265</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x10</th> <td>   -0.2149</td> <td>    3.493</td> <td>   -0.062</td> <td> 0.951</td> <td>   -7.082</td> <td>    6.652</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x11</th> <td>   -2.0123</td> <td>    1.577</td> <td>   -1.276</td> <td> 0.203</td> <td>   -5.112</td> <td>    1.087</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x12</th> <td>    0.4265</td> <td>    1.372</td> <td>    0.311</td> <td> 0.756</td> <td>   -2.271</td> <td>    3.124</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x13</th> <td>   -3.5219</td> <td>    2.076</td> <td>   -1.696</td> <td> 0.091</td> <td>   -7.604</td> <td>    0.560</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>166.428</td> <th>  Durbin-Watson:     </th> <td>   0.083</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 816.332</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td> 1.731</td>  <th>  Prob(JB):          </th> <td>5.44e-178</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td> 9.042</td>  <th>  Cond. No.          </th> <td>    9.81</td> \n",
              "</tr>\n",
              "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                                 OLS Regression Results                                \n",
              "=======================================================================================\n",
              "Dep. Variable:                      y   R-squared (uncentered):                   0.104\n",
              "Model:                            OLS   Adj. R-squared (uncentered):              0.075\n",
              "Method:                 Least Squares   F-statistic:                              3.507\n",
              "Date:                Mon, 11 Oct 2021   Prob (F-statistic):                    3.41e-05\n",
              "Time:                        18:36:47   Log-Likelihood:                         -1842.0\n",
              "No. Observations:                 404   AIC:                                      3710.\n",
              "Df Residuals:                     391   BIC:                                      3762.\n",
              "Df Model:                          13                                                  \n",
              "Covariance Type:            nonrobust                                                  \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "x1            -0.2469      1.565     -0.158      0.875      -3.323       2.830\n",
              "x2                  0      1.789          0      1.000      -3.518       3.518\n",
              "x3                 -0      2.325         -0      1.000      -4.571       4.571\n",
              "x4             0.3420      1.203      0.284      0.776      -2.023       2.707\n",
              "x5                 -0      2.544         -0      1.000      -5.002       5.002\n",
              "x6             2.8508      1.667      1.710      0.088      -0.426       6.128\n",
              "x7                 -0      2.112         -0      1.000      -4.152       4.152\n",
              "x8                 -0      2.334         -0      1.000      -4.589       4.589\n",
              "x9                 -0      3.186         -0      1.000      -6.265       6.265\n",
              "x10           -0.2149      3.493     -0.062      0.951      -7.082       6.652\n",
              "x11           -2.0123      1.577     -1.276      0.203      -5.112       1.087\n",
              "x12            0.4265      1.372      0.311      0.756      -2.271       3.124\n",
              "x13           -3.5219      2.076     -1.696      0.091      -7.604       0.560\n",
              "==============================================================================\n",
              "Omnibus:                      166.428   Durbin-Watson:                   0.083\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              816.332\n",
              "Skew:                           1.731   Prob(JB):                    5.44e-178\n",
              "Kurtosis:                       9.042   Cond. No.                         9.81\n",
              "==============================================================================\n",
              "\n",
              "Warnings:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIRLddf65hVs"
      },
      "source": [
        "1.Обычная линейная регрессия обучилась методом наименьших квадратов буз регуляризации. Практически не наблюдается занулившихся весов. Наибольшую важность модель выделила в признаке LSTAT, если сравнивать по абсолютной величине весов, и наименьшую важность в признаке INDUS. Без регуляризации такая модель может переобучиться и не наказывать за большие веса параметров регрессии. Модель не отбирает никакие признаки. Таким образом на обучающей выборке будет хорошее качество, но плохое на тестовой. Перед обучением признаки были отмасштабированы.\n",
        "\n",
        "2.При использовании Ridge регрессии можно заметить такую же тенденцию в весах как и при использовании обычной линейной регрессии, но немного видно, что веса были оштрафованы и немного ниже, чем при использовании обычной регрессии, так как веса не так больши по абсолютной величине. Наибольшую важность модель выделила в признаке LSTAT, если сравнивать по абсолютной величине весов, и наименьшую важность в признаке INDUS. Использовалась l1 регуляризация.\n",
        "\n",
        "3.При использовании Lasso регрессии некоторые веса занулились. Так как мы использовали l2 регуляризацию, модель отобрала наиболее информативные признаки.\n",
        "К информативным моджно отнести: CRIM, CHAS, RM, TAX, PTRATIO, B, LSTAT. Самый важный из них RM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYjdNZSA5hVs"
      },
      "source": [
        "---\n",
        "#### 4. [1 point] \n",
        "Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions. \n",
        "It's enough to apply to one of the models above (e.g simple linear regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhIOXfUd5hVs"
      },
      "source": [
        "# your code here \n",
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgP0HMW65hVs"
      },
      "source": [
        "```your conclusions here```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZaGOieH5hVt"
      },
      "source": [
        "---\n",
        "#### 5. [1 point] \n",
        "Find the best (in terms of RMSE) $\\alpha$ for Ridge regression using cross-validation with 5 folds. You must select values from range $[10^{-4}, 10^{3}]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41eVDcBg5hVt",
        "outputId": "ddde6ba1-4f14-41be-d21e-ffaad25e22e0"
      },
      "source": [
        "# your code here \n",
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import decimal\n",
        "ridge = Ridge()\n",
        "params = {'alpha' : np.arange(10**-4, 10**3, 10**-2)}\n",
        "BestRidge = GridSearchCV(ridge, params, scoring= 'neg_root_mean_squared_error', cv=5)\n",
        "BestRidge.fit(X_train, y_train)\n",
        "print(BestRidge.best_estimator_)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge(alpha=8.2101, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbBozTGMMkex"
      },
      "source": [
        "Данный метод работает достаточно долго, чтобы обучать модели с перебором весов.\n",
        "Найденный параметр равен alpha=8.2101."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl82Jzvz5hVt"
      },
      "source": [
        "---\n",
        "## Gradient descent\n",
        "\n",
        "#### 6. [3.5 points] \n",
        "**Implement a linear regression model for the MSE loss function, trained by gradient descent.**\n",
        "\n",
        "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
        "\n",
        "* checking for the Euclidean norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
        "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
        "\n",
        "You need to implement:\n",
        "\n",
        "* Full gradient descent:\n",
        "\n",
        "$$\n",
        "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
        "$$\n",
        "\n",
        "* Stochastic Gradient Descent:\n",
        "\n",
        "$$\n",
        "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
        "$$\n",
        "\n",
        "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
        "\n",
        "* Momentum method:\n",
        "\n",
        "$$\n",
        "h_0 = 0, \\\\\n",
        "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} q_{i_{k}} (w_{k}), \\\\\n",
        "w_{k + 1} = w_{k} - h_{k + 1}.\n",
        "$$\n",
        "\n",
        "Exponentially weighed averages can provide a better estimate which is closer to the actual gradient.\n",
        "\n",
        "\n",
        "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
        "\n",
        "You need to initialize the weights with a zero or random (from a normal distribution) vector. The following is a template class that needs to contain the code implementing all variations of the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nndw0X7L5hVu"
      },
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class LinReg(BaseEstimator):\n",
        "    def __init__(self, delta=1.0, gd_type='Momentum', \n",
        "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3):\n",
        "        \"\"\"\n",
        "        gd_type: str\n",
        "            'GradientDescent', 'StochasticDescent', 'Momentum'\n",
        "        delta: float\n",
        "            proportion of object in a batch (fot stochastic GD)\n",
        "        tolerance: float\n",
        "            for stopping gradient descent\n",
        "        max_iter: int\n",
        "            maximum number of steps in gradient descent\n",
        "        w0: np.array of shape (d)\n",
        "            init weights\n",
        "        eta: float\n",
        "            learning rate\n",
        "        alpha: float\n",
        "            momentum coefficient\n",
        "        \"\"\"\n",
        "        \n",
        "        self.gd_type = gd_type\n",
        "        self.delta = delta\n",
        "        self.tolerance = tolerance\n",
        "        self.max_iter = max_iter\n",
        "        self.w0 = w0\n",
        "        if w0 is None:\n",
        "            self.w0 = np.zeros(X.shape[1]+1)\n",
        "        self.alpha = alpha\n",
        "        self.w = None\n",
        "        self.eta = eta\n",
        "        self.loss_history = None # list of loss function values at each training iteration\n",
        "        self.h = 0\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: self\n",
        "        \"\"\"\n",
        "        self.w = self.w0\n",
        "        X = np.c_[ X, np.ones(X.shape[0]) ]\n",
        "\n",
        "        w_new = self.w\n",
        "        self.loss_history = []\n",
        "        for i in range(self.max_iter):\n",
        "          grad = self.calc_gradient(X, y)\n",
        "          \n",
        "          if(self.gd_type == 'GradientDescent' or self.gd_type == 'StochasticDescent'):\n",
        "            w_new = self.w - self.eta * grad\n",
        "          else:\n",
        "            self.h = self.alpha * self.h + self.eta * grad\n",
        "            w_new = self.w - self.h\n",
        "\n",
        "          self.loss_history.append(self.calc_loss(X, y))\n",
        "          if(np.linalg.norm(w_new - self.w) < self.tolerance):\n",
        "            break\n",
        "          else:\n",
        "            self.w = w_new\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        if self.w is None:\n",
        "            raise Exception('Not trained yet')\n",
        "        if(X.shape[1] != len(self.w)):\n",
        "          X = np.c_[ X, np.ones(X.shape[0]) ]\n",
        "        return np.dot(X, self.w)\n",
        "        \n",
        "    \n",
        "    def calc_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: np.array of shape (d)\n",
        "        \"\"\"\n",
        "        if(self.gd_type == 'GradientDescent'):\n",
        "          return 2 * np.dot(np.transpose(X), (np.dot(X, self.w) - y)) / X.shape[0]\n",
        "\n",
        "        if(self.gd_type == 'StochasticDescent'):\n",
        "          batch_ind = np.random.choice(X.shape[0], round(self.delta * X.shape[0]), replace=False)\n",
        "          X_batch = X[batch_ind]\n",
        "          y_batch = y[batch_ind]\n",
        "          return 2 * np.dot(np.transpose(X_batch), (np.dot(X_batch, self.w) - y_batch)) / X_batch.shape[0]\n",
        "\n",
        "        if(self.gd_type == 'Momentum'):\n",
        "          batch_ind = np.random.choice(X.shape[0], round(self.delta * X.shape[0]), replace=False)\n",
        "          X_batch = X[batch_ind]\n",
        "          y_batch = y[batch_ind]\n",
        "          return 2 * np.dot(np.transpose(X_batch), (np.dot(X_batch, self.w) - y_batch)) / X_batch.shape[0]\n",
        "\n",
        "\n",
        "    def calc_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: float \n",
        "        \"\"\" \n",
        "        return np.sum((self.predict(X) - y)  ** 2) / X.shape[0]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMOgoNYZ5hVv"
      },
      "source": [
        "#### 7. [1 points] \n",
        "Train and validate \"hand-written\" model (simple linear regression) on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na6qHicV5hVv",
        "outputId": "62615408-2d82-4222-e392-7e8bee57eaab"
      },
      "source": [
        "# your code here \n",
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "a = []\n",
        "for i in range(1000):\n",
        "  my_reg = LinReg(max_iter = 1000 + i, gd_type='GradientDescent')\n",
        "  my_reg.fit(X_train, y_train)\n",
        "  a.append(mean_squared_error(my_reg.predict(X_test), y_test) ** 0.5)\n",
        "\n",
        "print('The least on \"hand-written\" model RMSE:', np.min(a))\n",
        "\n",
        "sklearn_reg = LinearRegression()\n",
        "sklearn_reg.fit(X_train, y_train)\n",
        "print('The least on sklearn model RMSE:', mean_squared_error(sklearn_reg.predict(X_test), y_test) ** 0.5)\n",
        "print('The least on stats model RMSE is simililar to sklearn')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The least on \"hand-written\" model MSE: 5.791882477220084\n",
            "The least on sklearn model MSE: 5.783509315085134\n",
            "The least on stats model MSE is simililar to sklearn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "cD-vhKsuY7tt",
        "outputId": "62fdc83b-8029-477c-cfa7-932c67008d50"
      },
      "source": [
        "print(\"The best RMSE on hand-written model:\", np.min(a))\n",
        "print(\"Iteration with best MSE on hand-written model:\", range(1000)[np.argmin(a)] + 1000)\n",
        "plt.figure(figsize=(5, 6))\n",
        "plt.plot(a)\n",
        "\n",
        "a = []\n",
        "for i in np.arange(0.1, 1.1, 0.1):\n",
        "  my_reg = LinReg(alpha = i)\n",
        "  my_reg.fit(X_train, y_train)\n",
        "  a.append(mean_squared_error(my_reg.predict(X_test), y_test) ** 0.5)\n",
        "\n",
        "print(\"The best RMSE on hand-written Momentum model:\", np.min(a))\n",
        "print(\"The best alpha on hand-written Momentum model:\", np.arange(0.1, 1, 0.1)[np.argmin(a)])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best MSE on hand-written model: 5.791882477220084\n",
            "Iteration with best MSE on hand-written model: 1999\n",
            "The best MSE on hand-written Momentum model: 5.783737963384184\n",
            "The best alpha on hand-written Momentum model: 0.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x432 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgOku-SW5hVw"
      },
      "source": [
        "Для приведенных моделей Sklearn и StatsModels показали более хорошее качество, однако hand written модель при большем количество итераций могла бы показывать примерно такое же качество. Как описано на графике выше, ошибка с числом итераций снижалась.\n",
        "Количество итераций в градиентном спуске снижает ошибку до того момента пока мы не попадаем в малую окрестность минимума нашей функции потерь. Далее из-за константного шага модель либо попадает в минимум, либо \"кружит\" в этой окрестности. Так получается, что мало итераций оставляет большую ошибку, большое приводит к бесполезному вычислению градиентов при константном шаге.\n",
        "Коэфициент альфа при Momentum реализации( на сколько я понял, ведь этот параметр так называется в реализации) то при низком значении модель не получает никакого выигрыша в ошибке, при слошком большом происходит переполнение.\n",
        "Как и ожидалось библиотечные модели показывают лучшее качество, однако написанная отстает по качеству на одну сотую по MSE, что говорит о ее качестве."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9ESWpom5hVw"
      },
      "source": [
        "#### 8. [1 points] \n",
        "Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD and Momentum. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
        "\n",
        "Don't forget about what *beautiful* graphics should look like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypEzxBUA5hVw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "d688e7c6-b727-4e83-9e65-61d5d431e225"
      },
      "source": [
        "# your code here \n",
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "my_reg = LinReg(max_iter = 1500, gd_type='GradientDescent')\n",
        "my_reg.fit(X_train, y_train)\n",
        "print('MSE on 1500 iteration', mean_squared_error(my_reg.predict(X_test), y_test)**0.5)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(my_reg.loss_history)\n",
        "ax.set_xlabel('iterations')\n",
        "ax.set_ylabel('loss')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE on 1500 iteration 5.821578365471962\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "vvt4HCCdxaO9",
        "outputId": "b74d5cd9-31a0-4a82-efba-95d12bda4ce0"
      },
      "source": [
        "my_reg = LinReg(max_iter = 1500, gd_type='StochasticDescent', delta=0.15)\n",
        "my_reg.fit(X_train, y_train)\n",
        "print('MSE on 1500 iteration', mean_squared_error(my_reg.predict(X_test), y_test)**0.5)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(my_reg.loss_history)\n",
        "ax.set_xlabel('iterations')\n",
        "ax.set_ylabel('loss')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE on 1500 iteration 5.8258321129036075\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "metadata": {},
          "execution_count": 104
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "fXWAmWLTyy-i",
        "outputId": "650201e2-1660-47e7-d1c9-7e58230217f5"
      },
      "source": [
        "my_reg = LinReg(max_iter = 1500, gd_type='Momentum', delta=0.3, alpha = 0.6)\n",
        "my_reg.fit(X_train, y_train)\n",
        "print('MSE on 1500 iteration', mean_squared_error(my_reg.predict(X_test), y_test)**0.5)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(my_reg.loss_history)\n",
        "ax.set_xlabel('iterations')\n",
        "ax.set_ylabel('loss')"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE on 1500 iteration 5.75158503536169\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "metadata": {},
          "execution_count": 111
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf0HWeTj5hVw"
      },
      "source": [
        "Обычная линейная регрессия с градиентным спуском показывает один стабильный результат потерь. При этом стохастический градиентный спуск и моментум чувствительны к случайности, так как там формируется случайный батч для вычисления на нем градиента. \n",
        "От случайности зависит и само обучение двух последних методов, так как в батч может не попасть аномальные объекты или выбросы, тогда обучение произойдет лучше, чем при полном вычислении всех градиентов.\n",
        "При формировании батчей модель лучше понимает тенденцию всей выборки и это намного экономит вычисления. Однако такие модели не стабильны и привязаны к случайности."
      ]
    }
  ]
}
